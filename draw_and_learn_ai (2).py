# -*- coding: utf-8 -*-
"""Draw_and_Learn_AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pGYoJmiDj1QaY6V2pcZnldqKFW_SOO2h
"""

from google.colab import drive
drive.mount('/content/drive')

# ✅ UPDATE this if your ZIP is elsewhere
zip_path = "/content/drive/MyDrive/sketchy data.zip"
extract_dir ="/content/SketchyDatabase"

import zipfile, os

if not os.path.exists(extract_dir):
    os.makedirs(extract_dir)
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)
    print(f"✅ Extracted to {extract_dir}")
else:
    print("📂 Already extracted")

# Fix if ZIP extracts without 'temp_extraction' folder
extracted_root = "/content/SketchyDatabase"
expected_path = os.path.join(extracted_root, "temp_extraction", "256x256", "splitted_sketches")

if not os.path.exists(expected_path):
    # Sometimes all content is just directly inside 'SketchyDatabase'
    # So we create the expected folder structure manually
    os.makedirs(expected_path)
    for item in os.listdir(extracted_root):
        if item.endswith('.txt') or item == "temp_extraction":
            continue
        item_path = os.path.join(extracted_root, item)
        if os.path.isdir(item_path):
            shutil.move(item_path, os.path.join(expected_path, item))

src_root = "/content/SketchyDatabase"
dest_root = "/content/SketchyDatabase/organized_dataset"
train_dir = '/content/SketchyDatabase/organized_dataset/train'
test_dir = '/content/SketchyDatabase/organized_dataset/test'

import shutil

def organize_sketchy_dataset(src_root, dest_root):
    splits = ['train', 'test']
    for split in splits:
        split_path = os.path.join(src_root, 'temp_extraction', '256x256', 'splitted_sketches', split)

        for folder in os.listdir(split_path):
            folder_path = os.path.join(split_path, folder)
            if not os.path.isdir(folder_path):
                continue

            for class_name in os.listdir(folder_path):
                class_path = os.path.join(folder_path, class_name)
                if not os.path.isdir(class_path):
                    continue

                dest_class_folder = os.path.join(dest_root, split, class_name)
                os.makedirs(dest_class_folder, exist_ok=True)

                for img_file in os.listdir(class_path):
                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                        src_img = os.path.join(class_path, img_file)
                        dest_img = os.path.join(dest_class_folder, img_file)
                        shutil.copy2(src_img, dest_img)

    print("✅ Dataset organized into train/test/class folders!")

# Call the function
organize_sketchy_dataset('SketchyDatabase', 'SketchyDatabase/organized_dataset')

from collections import defaultdict

data_path = 'SketchyDatabase/organized_dataset/train'
class_counts = defaultdict(int)

for class_name in os.listdir(data_path):
    class_dir = os.path.join(data_path, class_name)
    if os.path.isdir(class_dir):
        class_counts[class_name] = len(os.listdir(class_dir))

print(f"📦 Total classes found: {len(class_counts)}")
print("🔍 Sample class distribution (first 10):")
for i, (cls, count) in enumerate(class_counts.items()):
    if i >= 10:
        break
    print(f"  - {cls}: {count} images")

import matplotlib.pyplot as plt
import random
from PIL import Image
import os

def show_random_class_samples(data_dir, num_classes=8, images_per_class=1):
    class_folders = random.sample(os.listdir(data_dir), num_classes)

    plt.figure(figsize=(images_per_class * 2.5, num_classes * 2.5))

    img_count = 1
    for class_idx, class_name in enumerate(class_folders):
        class_path = os.path.join(data_dir, class_name)
        image_files = random.sample(
            [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.png'))],
            images_per_class
        )

        for image_file in image_files:
            img_path = os.path.join(class_path, image_file)
            image = Image.open(img_path).convert("RGB")

            plt.subplot(num_classes, images_per_class, img_count)
            plt.imshow(image)
            plt.title(class_name, fontsize=8)
            plt.axis('off')
            img_count += 1

    plt.tight_layout()
    plt.show()

# Show 8 random classes with 1 image each
show_random_class_samples('SketchyDatabase/organized_dataset/train', num_classes=8, images_per_class=1)

from PIL import Image
import os

def resize_images_in_folder(folder_path, size=(256, 256)):
    count = 0
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.lower().endswith(('.jpg', '.png', '.jpeg')):
                img_path = os.path.join(root, file)
                try:
                    img = Image.open(img_path).convert("RGB")
                    img = img.resize(size, Image.Resampling.LANCZOS)
                    img.save(img_path)
                    count += 1
                except Exception as e:
                    print(f"⚠️ Error resizing: {img_path} | Error: {e}")
    print(f"✅ Resized {count} images to {size}")

# Apply to both train and test sets
resize_images_in_folder('SketchyDatabase/organized_dataset/train')
resize_images_in_folder('SketchyDatabase/organized_dataset/test')

from PIL import Image
import os

def check_image_sizes(folder_path, expected_size=(256, 256)):
    mismatched_images = []
    total_images = 0

    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.lower().endswith(('.jpg', '.png', '.jpeg')):
                total_images += 1
                img_path = os.path.join(root, file)
                try:
                    img = Image.open(img_path)
                    if img.size != expected_size:
                        mismatched_images.append((img_path, img.size))
                except Exception as e:
                    print(f"⚠️ Error opening image: {img_path} | Error: {e}")

    if mismatched_images:
        print(f"❌ Found {len(mismatched_images)} images NOT of size {expected_size}:")
        for path, size in mismatched_images[:10]:  # show first 10
            print(f" - {path} with size {size}")
    else:
        print(f"✅ All {total_images} images in '{folder_path}' are of size {expected_size}.")

# Check train folder
check_image_sizes('SketchyDatabase/organized_dataset/train')

# Check test folder
check_image_sizes('SketchyDatabase/organized_dataset/test')

"""---
# **Color-Fill Feature:**
"""

import os

base_dir = 'SketchyDatabase/organized_dataset/test'  # or 'train' if you want

if not os.path.exists(base_dir):
    print(f"Dataset folder not found: {base_dir}")
else:
    classes = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]
    print(f"Number of classes found: {len(classes)}")
    print("Sample classes:", classes[:10])  # print first 10 class names

import os
import random
import cv2
import numpy as np
import matplotlib.pyplot as plt

def load_random_image(base_dir='SketchyDatabase/organized_dataset', split='test', class_name=None):
    """
    Load random image from dataset.
    If class_name is None, pick random class.
    """
    split_dir = os.path.join(base_dir, split)

    if class_name is None:
        classes = [d for d in os.listdir(split_dir) if os.path.isdir(os.path.join(split_dir, d))]
        class_name = random.choice(classes)
    class_dir = os.path.join(split_dir, class_name)

    images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    img_file = random.choice(images)

    img_path = os.path.join(class_dir, img_file)
    print(f"Selected class: {class_name}, image: {img_file}")

    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    return img, class_name, img_path

def color_fill_on_sketch(img, click_point=None, fill_color=(0,255,0)):
    # Step 1: Threshold to binary (invert so lines are white)
    _, binary = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY_INV)

    # Step 2: Morphological closing to close small gaps
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))
    closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)

    # Step 3: Find contours
    contours, _ = cv2.findContours(closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # Default click_point to center if None
    if click_point is None:
        h, w = img.shape
        click_point = (w//2, h//2)

    # Step 4: Find contour that contains the point
    contour_idx = -1
    for i, cnt in enumerate(contours):
        if cv2.pointPolygonTest(cnt, click_point, False) >= 0:
            contour_idx = i
            break

    # Prepare color image for visualization
    color_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)

    if contour_idx != -1:
        # Create mask for floodFill (2 pixels bigger than image)
        h, w = img.shape
        mask = np.zeros((h+2, w+2), np.uint8)

        # Flood fill on the color image, starting from click_point
        cv2.floodFill(color_img, mask, click_point, fill_color, flags=cv2.FLOODFILL_FIXED_RANGE)
    else:
        print("Click point not inside any contour! No fill applied.")

    # Visualization
    plt.figure(figsize=(15,5))

    plt.subplot(1,4,1)
    plt.title("Original Sketch")
    plt.imshow(img, cmap='gray')
    plt.scatter([click_point[0]], [click_point[1]], c='red', s=50)
    plt.axis('off')

    plt.subplot(1,4,2)
    plt.title("Binary (Inverted)")
    plt.imshow(binary, cmap='gray')
    plt.axis('off')

    plt.subplot(1,4,3)
    plt.title("After Morph Closing")
    plt.imshow(closed, cmap='gray')
    plt.axis('off')

    plt.subplot(1,4,4)
    plt.title("Color Fill Result")
    plt.imshow(cv2.cvtColor(color_img, cv2.COLOR_BGR2RGB))
    plt.axis('off')

    plt.show()

# Example usage:
# Load random image from class 'cat' in test split
img, classname, imgpath = load_random_image(class_name='cat', split='test')

# Pick click point somewhere near center, or you can choose manually like (100,100)
click_point = (img.shape[1]//2, img.shape[0]//2)

# # Run color fill on loaded image
# color_fill_on_sketch(img, click_point=click_point, fill_color=(255, 0, 0))  # Fill blue color

# Example: convert hex "#00FF00" (green) to BGR
hex_color = "#00FF00"
rgb_color = tuple(int(hex_color[i:i+2], 16) for i in (1, 3, 5))
bgr_color = (rgb_color[2], rgb_color[1], rgb_color[0])
color_fill_on_sketch(img, click_point=click_point, fill_color=bgr_color)

import os
import cv2
import numpy as np
import random
import matplotlib.pyplot as plt
from ipywidgets import Dropdown, ColorPicker, IntText, Button, VBox, HBox, Output
from IPython.display import display, clear_output

# === Setup Paths ===
BASE_DIR = 'SketchyDatabase/organized_dataset/test'
CLASSES = sorted([d for d in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, d))])

# === UI Widgets ===
dropdown = Dropdown(options=CLASSES, description='Class:')
color_picker = ColorPicker(description='Pick Color', value='#ff0000')
x_input = IntText(description='X:')
y_input = IntText(description='Y:')
fill_btn = Button(description='Apply Fill', button_style='info')
load_btn = Button(description='Load Image', button_style='primary')
out = Output()

# === Globals ===
img_gray, img_color = None, None
img_path = None

# === Load Image Function ===
def load_random_image(class_name):
    global img_gray, img_color, img_path

    class_dir = os.path.join(BASE_DIR, class_name)
    img_file = random.choice([f for f in os.listdir(class_dir) if f.endswith('.png')])
    img_path = os.path.join(class_dir, img_file)

    img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    img_color = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2BGR)

    with out:
        clear_output()
        print(f"Loaded image: {img_file}")
        plt.figure(figsize=(6,6))
        plt.imshow(img_gray, cmap='gray')
        plt.title("Original Sketch (Pick fill point)")
        plt.axis('off')
        plt.show()

# === Color Fill Function ===
def apply_flood_fill(x, y, hex_color):
    if img_color is None:
        with out:
            clear_output()
            print("Please load an image first.")
        return

    h, w = img_color.shape[:2]
    if not (0 <= x < w and 0 <= y < h):
        with out:
            clear_output()
            print("Invalid coordinates.")
        return

    # Convert hex to BGR
    rgb = tuple(int(hex_color[i:i+2], 16) for i in (1, 3, 5))
    bgr = (rgb[2], rgb[1], rgb[0])

    output = img_color.copy()
    mask = np.zeros((h+2, w+2), np.uint8)

    cv2.floodFill(output, mask, (x, y), bgr, loDiff=(20,20,20), upDiff=(20,20,20),
                  flags=4 | cv2.FLOODFILL_FIXED_RANGE)

    with out:
        clear_output()
        print(f"Flood fill at ({x},{y}) with color {hex_color}")
        plt.figure(figsize=(6,6))
        plt.imshow(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))
        plt.axis('off')
        plt.title("Color-Filled Sketch")
        plt.show()

# === Button Callbacks ===
def on_load_clicked(_):
    load_random_image(dropdown.value)

def on_fill_clicked(_):
    apply_flood_fill(x_input.value, y_input.value, color_picker.value)

# === Hook Up Buttons ===
load_btn.on_click(on_load_clicked)
fill_btn.on_click(on_fill_clicked)

# === Display UI ===
display(VBox([
    dropdown,
    HBox([load_btn, color_picker]),
    HBox([x_input, y_input, fill_btn]),
    out
]))

"""

---

# **Complete-the-Drawing**"""

import os, cv2, numpy as np, random

FULL_IMG_DIR = 'SketchyDatabase/organized_dataset/test'
PARTIAL_IMG_DIR = 'partial_sketches'
os.makedirs(PARTIAL_IMG_DIR, exist_ok=True)

def generate_partial_image(img, method='bottom_half'):
    h, w = img.shape
    mask = np.ones((h, w), dtype=np.uint8) * 255
    if method == 'bottom_half':
        mask[h//2:, :] = 0
    elif method == 'left_half':
        mask[:, :w//2] = 0
    elif method == 'random_patch':
        x, y = random.randint(0, w//2), random.randint(0, h//2)
        mask[y:y+h//3, x:x+w//3] = 0
    return cv2.bitwise_and(img, img, mask=mask)

for class_folder in os.listdir(FULL_IMG_DIR):
    class_path = os.path.join(FULL_IMG_DIR, class_folder)
    if not os.path.isdir(class_path): continue

    for fname in os.listdir(class_path):
        if not fname.endswith('.png'): continue
        full_path = os.path.join(class_path, fname)
        img = cv2.imread(full_path, 0)
        method = random.choice(['bottom_half', 'left_half', 'random_patch'])
        partial = generate_partial_image(img, method)
        save_class_dir = os.path.join(PARTIAL_IMG_DIR, class_folder)
        os.makedirs(save_class_dir, exist_ok=True)
        save_path = os.path.join(save_class_dir, fname)
        cv2.imwrite(save_path, partial)

print("✅ Partial sketches saved in:", PARTIAL_IMG_DIR)

import os

partial_root = 'partial_sketches'

if os.path.exists(partial_root):
    print(f"✅ Found 'partial_sketches/' folder at: {partial_root}")
    classes = os.listdir(partial_root)
    print(f"🔍 Total classes: {len(classes)}")
    for cls in classes[:10]:  # Show first 10 classes
        cls_path = os.path.join(partial_root, cls)
        count = len([f for f in os.listdir(cls_path) if f.endswith('.png')])
        print(f"  - {cls}: {count} partial images")
else:
    print("❌ 'partial_sketches/' folder not found.")

import matplotlib.pyplot as plt
import random
import cv2

def show_random_partials(partial_dir='partial_sketches', num_classes=4, imgs_per_class=2):
    classes = random.sample(os.listdir(partial_dir), num_classes)
    plt.figure(figsize=(imgs_per_class * 3, num_classes * 3))

    img_count = 1
    for cls in classes:
        cls_path = os.path.join(partial_dir, cls)
        imgs = random.sample([f for f in os.listdir(cls_path) if f.endswith('.png')], imgs_per_class)
        for fname in imgs:
            path = os.path.join(cls_path, fname)
            img = cv2.imread(path, 0)
            plt.subplot(num_classes, imgs_per_class, img_count)
            plt.imshow(img, cmap='gray')
            plt.title(f"{cls}", fontsize=8)
            plt.axis('off')
            img_count += 1

    plt.tight_layout()
    plt.show()

show_random_partials()

import torch
from torchvision import models, transforms
from sklearn.metrics.pairwise import cosine_similarity
from PIL import Image

# --- VGG Feature Extractor (conv5_3) ---
class VGGFeatureExtractor(torch.nn.Module):
    def __init__(self):
        super().__init__()
        vgg = models.vgg16(pretrained=True)
        self.features = torch.nn.Sequential(*list(vgg.features.children())[:30])  # conv5_3 layer
        self.transforms = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])

    def forward(self, image):
        image = self.transforms(image).unsqueeze(0)
        with torch.no_grad():
            features = self.features(image)
        return features.view(-1)

def match_shapes(img1, img2):
    cnts1, _ = cv2.findContours(img1, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts2, _ = cv2.findContours(img2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if cnts1 and cnts2:
        return cv2.matchShapes(cnts1[0], cnts2[0], 1, 0.0)
    return 1.0  # worst match if contours not found

def evaluate_completion(partial_path, completed_path, true_path):
    print("📄 Evaluating drawing...")

    # Load grayscale images
    img_partial = cv2.imread(partial_path, 0)
    img_completed = cv2.imread(completed_path, 0)
    img_true = cv2.imread(true_path, 0)

    # --- Shape Similarity ---
    shape_score = match_shapes(img_completed, img_true)

    # --- VGG Embedding Similarity ---
    extractor = VGGFeatureExtractor()
    img_completed_rgb = Image.open(completed_path).convert("RGB")
    img_true_rgb = Image.open(true_path).convert("RGB")
    emb_completed = extractor(img_completed_rgb)
    emb_true = extractor(img_true_rgb)
    cos_sim = cosine_similarity(emb_completed.view(1, -1), emb_true.view(1, -1))[0][0]

    print(f"🔍 Shape Match Score (lower is better): {shape_score:.4f}")
    print(f"🔍 Embedding Cosine Similarity (higher is better): {cos_sim:.4f}")

    # --- Final Decision ---
    if shape_score < 0.25 and cos_sim > 0.85:
        print("✅ Drawing Completed Correctly!")
    else:
        print("❌ Drawing Incomplete or Incorrect.")

import matplotlib.pyplot as plt
import cv2
import os

def display_before_after(partial_path, completed_path):
    img_partial = cv2.imread(partial_path, 0)
    img_completed = cv2.imread(completed_path, 0)

    plt.figure(figsize=(10,5))

    plt.subplot(1,2,1)
    plt.imshow(img_partial, cmap='gray')
    plt.title("Partial Sketch")
    plt.axis('off')

    plt.subplot(1,2,2)
    plt.imshow(img_completed, cmap='gray')
    plt.title("Completed Sketch")
    plt.axis('off')

    plt.show()

import os

class_name = "bicycle"
test_dir = f"SketchyDatabase/organized_dataset/test/{class_name}"

if os.path.exists(test_dir):
    filenames = sorted([f for f in os.listdir(test_dir) if f.endswith('.png')])
    print(f"✅ Found {len(filenames)} images for class '{class_name}'.")
    print("📄 Sample filenames:", filenames[:5])  # Print first 5
else:
    print("❌ Class folder not found:", test_dir)

class_name = "bicycle"
filename = "n02834778_10158-2.png"  # Make sure this file exists in both partial_sketches and full dataset

partial_path = f"partial_sketches/{class_name}/{filename}"
completed_path = f"SketchyDatabase/organized_dataset/test/{class_name}/{filename}"  # using ground truth as "completed" for now
true_path = completed_path  # ground truth

# Display partial and completed sketches
display_before_after(partial_path, completed_path)

# Evaluate completion
evaluate_completion(partial_path, completed_path, true_path)

import ipywidgets as widgets
from IPython.display import display
import os

# Get list of classes from partial sketches folder
partial_root = 'partial_sketches'
class_names = sorted([d for d in os.listdir(partial_root) if os.path.isdir(os.path.join(partial_root, d))])

# Dropdown for class selection
class_dropdown = widgets.Dropdown(
    options=class_names,
    description='Class:',
    disabled=False,
)

# Dropdown for filename - will update based on class selection
filename_dropdown = widgets.Dropdown(
    options=[],
    description='Filename:',
    disabled=False,
)

# Function to update filenames based on selected class
def update_filenames(change):
    selected_class = change['new']
    class_dir = os.path.join(partial_root, selected_class)
    files = sorted([f for f in os.listdir(class_dir) if f.endswith('.png')])
    filename_dropdown.options = files

# Link the update function to class dropdown
class_dropdown.observe(update_filenames, names='value')

# Initial update to populate filenames for default class
update_filenames({'new': class_dropdown.value})

# Button to trigger display + evaluation
button = widgets.Button(description="Show and Evaluate")

output = widgets.Output()

def on_button_clicked(b):
    with output:
        output.clear_output()
        cls = class_dropdown.value
        fname = filename_dropdown.value

        partial_path = f"partial_sketches/{cls}/{fname}"
        completed_path = f"SketchyDatabase/organized_dataset/test/{cls}/{fname}"
        true_path = completed_path

        if os.path.exists(partial_path) and os.path.exists(completed_path):
            display_before_after(partial_path, completed_path)
            evaluate_completion(partial_path, completed_path, true_path)
        else:
            print("Paths don't exist. Check class and filename.")

button.on_click(on_button_clicked)

# Display widgets
display(class_dropdown, filename_dropdown, button, output)

import os
print(os.listdir('SketchyDatabase/organized_dataset/train'))

import os
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, models, transforms
from tqdm import tqdm

# --- Step 1: Device setup ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Enable cuDNN benchmark for faster convolutions with fixed input size
torch.backends.cudnn.benchmark = True

# --- Step 2: Dataset paths ---
train_dir = 'SketchyDatabase/organized_dataset/train'
val_dir = 'SketchyDatabase/organized_dataset/test'

# --- Step 3: ImageNet normalization stats ---
imagenet_mean = [0.485, 0.456, 0.406]
imagenet_std = [0.229, 0.224, 0.225]

# --- Step 4: Define transforms ---
train_transforms = transforms.Compose([
    transforms.RandomRotation(15),
    transforms.RandomHorizontalFlip(),
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
    transforms.ToTensor(),
    transforms.Normalize(imagenet_mean, imagenet_std)
])

val_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(imagenet_mean, imagenet_std)
])

# --- Step 5: Load datasets ---
train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)
val_dataset = datasets.ImageFolder(val_dir, transform=val_transforms)

# --- Step 6: Create dataloaders ---
batch_size = 32  # Reduced from 64 to save memory and avoid OOM
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)

# --- Step 7: Number of classes ---
num_classes = len(train_dataset.classes)
print(f"Total classes: {num_classes}")

# --- Step 8: Load pretrained VGG16 model ---
model = models.vgg16(pretrained=True)

# --- Step 9: Freeze feature extractor layers ---
for param in model.features.parameters():
    param.requires_grad = False

# Freeze first 5 layers of classifier except the last layer
for param in model.classifier[:-1].parameters():
    param.requires_grad = False

# Replace final classifier layer
model.classifier[6] = nn.Linear(4096, num_classes)

model = model.to(device)

# --- Step 10: Loss, optimizer, scheduler ---
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)

# --- Step 11: Mixed precision scaler ---
scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())

# --- Step 12: Gradient accumulation setup ---
accumulation_steps = 2  # Effective batch size = batch_size * accumulation_steps (simulate batch_size=64)

# --- Step 13: Checkpoint and early stopping ---
checkpoint_path = "best_model.pth"
start_epoch = 0
best_val_acc = 0.0
best_val_loss = float('inf')
patience = 7
epochs_no_improve = 0

if os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_epoch = checkpoint['epoch'] + 1
    best_val_acc = checkpoint.get('best_val_acc', 0.0)
    best_val_loss = checkpoint.get('best_val_loss', float('inf'))
    print(f"Resuming training from epoch {start_epoch} with best val acc {best_val_acc:.4f} and val loss {best_val_loss:.4f}")

# --- Step 14: Training loop ---
num_epochs = 30

for epoch in range(start_epoch, num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    train_total = 0

    optimizer.zero_grad()
    loop = tqdm(enumerate(train_loader), total=len(train_loader), desc=f"Epoch {epoch+1}/{num_epochs} [Training]")
    for batch_idx, (images, labels) in loop:
        images, labels = images.to(device), labels.to(device)

        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss = loss / accumulation_steps  # Normalize loss for gradient accumulation

        scaler.scale(loss).backward()

        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

        train_loss += loss.item() * images.size(0) * accumulation_steps  # multiply back the loss
        _, predicted = outputs.max(1)
        train_correct += predicted.eq(labels).sum().item()
        train_total += labels.size(0)

        loop.set_postfix(loss=loss.item() * accumulation_steps, acc=100.*train_correct/train_total)

    train_loss /= train_total
    train_acc = 100. * train_correct / train_total

    # Validation phase
    model.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item() * images.size(0)
            _, predicted = outputs.max(1)
            val_correct += predicted.eq(labels).sum().item()
            val_total += labels.size(0)

    val_loss /= val_total
    val_acc = 100. * val_correct / val_total

    print(f"Epoch {epoch+1} summary: Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%")

    scheduler.step(val_loss)

    # Early stopping check
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        epochs_no_improve = 0
    else:
        epochs_no_improve += 1
        print(f"Epochs without improvement: {epochs_no_improve}/{patience}")

    # Save checkpoint if improved val accuracy
    if val_acc > best_val_acc:
        print(f"Validation accuracy improved from {best_val_acc:.4f} to {val_acc:.4f}, saving model...")
        best_val_acc = val_acc
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_val_acc': best_val_acc,
            'best_val_loss': best_val_loss
        }, checkpoint_path)

    if epochs_no_improve >= patience:
        print(f"No improvement for {patience} epochs. Early stopping triggered.")
        break

import torch
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import time
import os

# --- Device Setup ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"✅ Using device: {device}")

# --- Paths ---
test_dir = 'SketchyDatabase/organized_dataset/test'
checkpoint_path = 'best_model.pth'

# --- ImageNet Normalization ---
imagenet_mean = [0.485, 0.456, 0.406]
imagenet_std = [0.229, 0.224, 0.225]

# --- Transforms ---
test_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(imagenet_mean, imagenet_std)
])

# --- Dataset and DataLoader ---
test_dataset = datasets.ImageFolder(test_dir, transform=test_transforms)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)

class_names = test_dataset.classes
num_classes = len(class_names)
print(f"🧪 Number of test classes: {num_classes}")

# --- Model Definition (Must match training config) ---
model = models.vgg16(pretrained=False)

# Freeze feature extractor parameters (to match training)
for param in model.features.parameters():
    param.requires_grad = False

# Freeze first 4 layers of classifier as done during training
for param in model.classifier[:4].parameters():
    param.requires_grad = False

# Modify final classifier layer
model.classifier[6] = torch.nn.Linear(4096, num_classes)

# --- Load Checkpoint ---
if os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"📦 Loaded model checkpoint from {checkpoint_path}")
else:
    raise FileNotFoundError(f"❌ Checkpoint not found at {checkpoint_path}")

model = model.to(device)

# --- Evaluation Function ---
def evaluate_model(model, dataloader, device):
    model.eval()
    all_preds = []
    all_labels = []
    total_time = 0.0
    total_samples = 0

    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)

            start_time = time.time()
            outputs = model(images)
            end_time = time.time()

            total_time += (end_time - start_time)
            total_samples += images.size(0)

            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    cm = confusion_matrix(all_labels, all_preds)
    report = classification_report(all_labels, all_preds, target_names=class_names, digits=4)
    avg_latency = total_time / total_samples
    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))

    print(f"✅ Test Accuracy: {accuracy * 100:.2f}%")
    print(f"⏱️ Avg. inference latency per image: {avg_latency * 1000:.2f} ms")
    print("\n📊 Classification Report:\n", report)

    return cm

# --- Run Evaluation ---
cm = evaluate_model(model, test_loader, device)

# --- Plot Confusion Matrix ---
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.tight_layout()
plt.show()

# Step 1: Precompute full sketch embeddings only once
print("Caching full sketch embeddings...")
full_embeddings = {}

for class_dir in os.listdir(full_sketch_dir):
    full_class_path = Path(full_sketch_dir) / class_dir
    if not full_class_path.is_dir():
        continue

    for file in full_class_path.iterdir():
        if file.suffix.lower() not in ['.png', '.jpg', '.jpeg']:
            continue
        img_id = file.stem.split('-')[0]
        key = f"{class_dir}/{img_id}"
        full_embeddings[key] = extract_embedding(str(file))
print(f"✅ Cached {len(full_embeddings)} full sketch embeddings.\n")

# Step 2: Loop through partial sketches
print("Evaluating partial sketches...")

for i, (img_tensor, label) in enumerate(partial_loader):
    img_tensor = img_tensor.to(device)
    label = label.to(device)

    with torch.no_grad():
        output = model(img_tensor)
        _, pred = torch.max(output, 1)

    all_preds.append(pred.item())
    all_labels.append(label.item())

    partial_img_path, _ = partial_dataset.samples[i]
    partial_path = Path(partial_img_path)
    class_name = partial_path.parts[1]
    filename = partial_path.stem.split('-')[0]
    key = f"{class_name}/{filename}"

    if key in full_embeddings:
        partial_emb = extract_embedding(partial_img_path)
        full_emb = full_embeddings[key]
        sim = F.cosine_similarity(partial_emb, full_emb).item()
        completed = sim >= similarity_threshold
        completion_results.append(completed)

        if printed_samples < max_samples_to_print:
            print(f"Sample {i+1}: True: {class_names[label.item()]} | Pred: {class_names[pred.item()]} | Similarity: {sim:.3f} | Completed: {completed}")
            printed_samples += 1
    else:
        completion_results.append(False)

import matplotlib.pyplot as plt
from PIL import Image

def display_before_after(partial_path, completed_path):
    partial_img = Image.open(partial_path).convert("RGB")
    completed_img = Image.open(completed_path).convert("RGB")

    fig, axs = plt.subplots(1, 2, figsize=(10, 5))
    axs[0].imshow(partial_img)
    axs[0].set_title("Partial Sketch")
    axs[0].axis('off')

    axs[1].imshow(completed_img)
    axs[1].set_title("Full Sketch")
    axs[1].axis('off')

    plt.show()

import torch
import torch.nn.functional as F

# Assuming 'model' and 'transform' are already defined as in your previous code

def extract_embedding(image_path):
    image = Image.open(image_path).convert("RGB")
    tensor = transform(image).unsqueeze(0).to(device)
    with torch.no_grad():
        features = model.features(tensor)
        pooled = F.adaptive_avg_pool2d(features, (7, 7))
        flat = torch.flatten(pooled, 1)
        return flat

def evaluate_completion(partial_path, completed_path, true_path):
    partial_emb = extract_embedding(partial_path)
    completed_emb = extract_embedding(completed_path)
    true_emb = extract_embedding(true_path)

    sim_partial_completed = F.cosine_similarity(partial_emb, completed_emb).item()
    sim_completed_true = F.cosine_similarity(completed_emb, true_emb).item()

    print(f"Similarity (Partial vs Completed): {sim_partial_completed:.3f}")
    print(f"Similarity (Completed vs True Full): {sim_completed_true:.3f}")

    threshold = 0.85
    if sim_completed_true >= threshold:
        print("✅ Completion is correct (above threshold)")
    else:
        print("❌ Completion is not accurate enough (below threshold)")

import os
print(os.listdir('partial_sketches/airplane'))

import cv2
import numpy as np
from matplotlib import pyplot as plt

def sketch_completion_cv(partial_sketch_path):
    # Load image in grayscale
    img = cv2.imread(partial_sketch_path, cv2.IMREAD_GRAYSCALE)

    # Invert: Sketch lines become white, background black
    img_inv = cv2.bitwise_not(img)

    # Morphological closing to close gaps in lines (dilate then erode)
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))
    closed = cv2.morphologyEx(img_inv, cv2.MORPH_CLOSE, kernel, iterations=2)

    # Invert back
    closed_inv = cv2.bitwise_not(closed)

    # Optional: Flood fill from background to remove small holes inside lines
    floodfilled = closed_inv.copy()
    h, w = closed_inv.shape
    mask = np.zeros((h+2, w+2), np.uint8)
    cv2.floodFill(floodfilled, mask, (0,0), 255)

    # Invert floodfilled image
    floodfill_inv = cv2.bitwise_not(floodfilled)

    # Combine floodfill inverse with closed_inv to fill holes inside lines
    completed = closed_inv | floodfill_inv

    # Show original and completed side by side
    plt.figure(figsize=(10,5))
    plt.subplot(1,2,1)
    plt.title("Partial Sketch")
    plt.imshow(img, cmap='gray')
    plt.axis('off')

    plt.subplot(1,2,2)
    plt.title("Completed Sketch (CV)")
    plt.imshow(completed, cmap='gray')
    plt.axis('off')
    plt.show()

    return completed

#Usage example:
completed_img = sketch_completion_cv('partial_sketches/airplane/n02691156_10151-4.png')





"""---
# **Model Design with VGG16.**
"""

import os
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, models, transforms
from PIL import Image
import numpy as np
from tqdm import tqdm

# --- Step 1: Device setup ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# --- Step 2: Dataset paths ---
train_dir = 'SketchyDatabase/organized_dataset/train'
test_dir = 'SketchyDatabase/organized_dataset/test'

# --- Step 3: ImageNet normalization stats ---
imagenet_mean = [0.485, 0.456, 0.406]
imagenet_std = [0.229, 0.224, 0.225]

# --- Step 4: Define transforms ---
train_transforms = transforms.Compose([
    transforms.RandomRotation(15),
    transforms.RandomHorizontalFlip(),
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
    transforms.ToTensor(),
    transforms.Normalize(imagenet_mean, imagenet_std)
])

test_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(imagenet_mean, imagenet_std)
])

# --- Step 5: Load datasets ---
train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)
test_dataset = datasets.ImageFolder(test_dir, transform=test_transforms)

# --- Step 6: Create dataloaders ---
batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

# --- Step 7: Get number of classes ---
num_classes = len(train_dataset.classes)
print(f"🧠 Total classes: {num_classes}")

# --- Step 8: Load pretrained VGG16 model ---
vgg16 = models.vgg16(pretrained=True)

# --- Step 9: Freeze feature extractor parameters ---
for param in vgg16.features.parameters():
    param.requires_grad = False

# --- Step 10: Modify classifier to output num_classes ---
vgg16.classifier[6] = nn.Linear(in_features=4096, out_features=num_classes)

# --- Step 11: Move model to device ---
vgg16 = vgg16.to(device)

# --- Step 12: Define preprocessing for inference or feature extraction ---
preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(imagenet_mean, imagenet_std)
])


# --- Step 13: Prediction function ---
def predict(image_path, model, preprocess, device):
    model.eval()
    image = Image.open(image_path).convert("RGB")
    input_tensor = preprocess(image).unsqueeze(0).to(device)
    with torch.no_grad():
        outputs = model(input_tensor)
        probabilities = torch.softmax(outputs, dim=1)
        conf_score, pred_idx = torch.max(probabilities, dim=1)
    return pred_idx.item(), conf_score.item()

# --- Step 14: Example usage ---
sample_image_path = "SketchyDatabase/organized_dataset/test/fan/n03271574_4558-3.png"  # Update to your actual image path
pred_class_idx, confidence = predict(sample_image_path, vgg16, preprocess, device)
print(f"Predicted class index: {pred_class_idx}, Confidence: {confidence:.4f}")

"""# **Training & Evaluation Code**"""

import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, models, transforms
from tqdm import tqdm
import os

# --- Device setup ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# --- Dataset and transforms ---
train_dir ='SketchyDatabase/organized_dataset/train'
val_dir = 'SketchyDatabase/organized_dataset/test'

imagenet_mean = [0.485, 0.456, 0.406]
imagenet_std = [0.229, 0.224, 0.225]

train_transforms = transforms.Compose([
    transforms.RandomRotation(15),
    transforms.RandomHorizontalFlip(),
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
    transforms.ToTensor(),
    transforms.Normalize(imagenet_mean, imagenet_std)
])

val_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(imagenet_mean, imagenet_std)
])

train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)
val_dataset = datasets.ImageFolder(val_dir, transform=val_transforms)

batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)

num_classes = len(train_dataset.classes)
print(f"Total classes: {num_classes}")

# --- Model setup ---
model = models.vgg16(pretrained=True)

# Freeze conv layers
for param in model.features.parameters():
    param.requires_grad = False

# Freeze first 4 classifier layers
for param in model.classifier[:4].parameters():
    param.requires_grad = False

# Replace final layer
model.classifier[6] = nn.Linear(4096, num_classes)

model = model.to(device)

# Loss, optimizer, scheduler
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)

# Mixed precision scaler
scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())

# Checkpoint params & early stopping
checkpoint_path = "best_model.pth"
start_epoch = 0
best_val_acc = 0.0
best_val_loss = float('inf')
patience = 7  # epochs to wait before early stop
epochs_no_improve = 0

if os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_epoch = checkpoint['epoch'] + 1
    best_val_acc = checkpoint.get('best_val_acc', 0.0)
    best_val_loss = checkpoint.get('best_val_loss', float('inf'))
    print(f"Resuming training from epoch {start_epoch} with best val acc {best_val_acc:.4f} and val loss {best_val_loss:.4f}")

num_epochs = 50

for epoch in range(start_epoch, num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    train_total = 0

    loop = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Training]")
    for images, labels in loop:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()

        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):
            outputs = model(images)
            loss = criterion(outputs, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        train_loss += loss.item() * images.size(0)
        _, predicted = outputs.max(1)
        train_correct += predicted.eq(labels).sum().item()
        train_total += labels.size(0)

        loop.set_postfix(loss=loss.item(), acc=100.*train_correct/train_total)

    train_loss /= train_total
    train_acc = 100. * train_correct / train_total

    # Validation phase
    model.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item() * images.size(0)
            _, predicted = outputs.max(1)
            val_correct += predicted.eq(labels).sum().item()
            val_total += labels.size(0)

    val_loss /= val_total
    val_acc = 100. * val_correct / val_total

    print(f"Epoch {epoch+1} summary: Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%")

    # Step scheduler by validation loss
    scheduler.step(val_loss)

    # Early stopping check
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        epochs_no_improve = 0
    else:
        epochs_no_improve += 1
        print(f"Epochs without improvement: {epochs_no_improve}/{patience}")

    # Save checkpoint if improved val accuracy
    if val_acc > best_val_acc:
        print(f"Validation accuracy improved from {best_val_acc:.4f} to {val_acc:.4f}, saving model...")
        best_val_acc = val_acc
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_val_acc': best_val_acc,
            'best_val_loss': best_val_loss
        }, checkpoint_path)

    if epochs_no_improve >= patience:
        print(f"No improvement for {patience} epochs. Early stopping triggered.")
        break

"""---
# **Testing of model/ Inference**
"""



import os
os.makedirs('checkpoints', exist_ok=True)

import torch
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import time
import os

# --- Device Setup ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"✅ Using device: {device}")

# --- Paths ---
test_dir = 'SketchyDatabase/organized_dataset/test'
checkpoint_path = 'best_model.pth'

# --- ImageNet Normalization ---
imagenet_mean = [0.485, 0.456, 0.406]
imagenet_std = [0.229, 0.224, 0.225]

# --- Transforms ---
test_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(imagenet_mean, imagenet_std)
])

# --- Dataset and DataLoader ---
test_dataset = datasets.ImageFolder(test_dir, transform=test_transforms)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)

class_names = test_dataset.classes
num_classes = len(class_names)
print(f"🧪 Number of test classes: {num_classes}")

# --- Model Definition (Must match training config) ---
model = models.vgg16(pretrained=False)
for param in model.features.parameters():
    param.requires_grad = False
for param in model.classifier[:4].parameters():
    param.requires_grad = False
model.classifier[6] = torch.nn.Linear(4096, num_classes)

# --- Load Checkpoint ---
if os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])  # ✅ use model_state_dict
    print(f"📦 Loaded model checkpoint from {checkpoint_path}")
else:
    raise FileNotFoundError(f"❌ Checkpoint not found at {checkpoint_path}")

model = model.to(device)

# --- Evaluation Function ---
def evaluate_model(model, dataloader, device):
    model.eval()
    all_preds = []
    all_labels = []
    total_time = 0.0
    total_samples = 0

    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)

            start_time = time.time()
            outputs = model(images)
            end_time = time.time()

            total_time += (end_time - start_time)
            total_samples += images.size(0)

            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    cm = confusion_matrix(all_labels, all_preds)
    report = classification_report(all_labels, all_preds, target_names=class_names, digits=4)
    avg_latency = total_time / total_samples
    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))

    print(f"✅ Test Accuracy: {accuracy * 100:.2f}%")
    print(f"⏱️ Avg. inference latency per image: {avg_latency * 1000:.2f} ms")
    print("\n📊 Classification Report:\n", report)

    return cm

# --- Run Evaluation ---
cm = evaluate_model(model, test_loader, device)

# --- Plot Confusion Matrix ---
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=False, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.tight_layout()
plt.show()



import torch
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models
from sklearn.metrics import accuracy_score
import os

# --- Device setup ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"✅ Using device: {device}")

# --- Paths ---
test_dir = 'SketchyDatabase/organized_dataset/test'
checkpoint_path = 'best_model.pth'

# --- Transforms (must match training) ---
imagenet_mean = [0.485, 0.456, 0.406]
imagenet_std = [0.229, 0.224, 0.225]

test_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(imagenet_mean, imagenet_std)
])

# --- Load test dataset ---
test_dataset = datasets.ImageFolder(test_dir, transform=test_transforms)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)

class_names = test_dataset.classes
num_classes = len(class_names)
print(f"📂 Number of test classes: {num_classes}")

# --- Load model architecture ---
model = models.vgg16(pretrained=False)

# Freeze layers just like during training
for param in model.features.parameters():
    param.requires_grad = False
for param in model.classifier[:4].parameters():
    param.requires_grad = False

# Update classifier for our dataset
model.classifier[6] = torch.nn.Linear(4096, num_classes)

# --- Load checkpoint ---
if os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"📦 Loaded model checkpoint from {checkpoint_path}")
else:
    raise FileNotFoundError(f"❌ Checkpoint file not found: {checkpoint_path}")

model = model.to(device)
model.eval()

# --- Inference and Accuracy Calculation ---
all_preds = []
all_labels = []

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.to(device)

        outputs = model(images)
        _, preds = torch.max(outputs, 1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

accuracy = accuracy_score(all_labels, all_preds)
print(f"✅ Test Accuracy: {accuracy * 100:.2f}%")